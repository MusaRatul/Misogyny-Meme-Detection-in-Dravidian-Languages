{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10580558,"sourceType":"datasetVersion","datasetId":6547966}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom tqdm import tqdm\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Define Dataset Class\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, image_dir, max_len, transform, has_labels):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.image_dir = image_dir\n        self.max_len = max_len\n        self.transform = transform\n        self.has_labels = has_labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = str(self.data.loc[idx, 'image_id'])\n        if not img_name.endswith(\".jpg\"):\n            img_name += \".jpg\"\n        img_path = os.path.join(self.image_dir, img_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        caption = self.data.loc[idx, 'transcriptions']\n\n        if self.transform:\n            image = self.transform(image)\n\n        inputs = self.tokenizer(\n            caption, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len\n        )\n\n        item = {\n            'image': image,\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n        }\n\n        if self.has_labels:\n            item['label'] = int(self.data.loc[idx, 'labels'])\n\n        return item\n\n# Load Dataset Function\ndef load_data(csv_path, image_dir, tokenizer, max_len, batch_size, has_labels=True):\n    data = pd.read_csv(csv_path)\n\n    if has_labels:\n        data['labels'] = data['labels'].astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    dataset = MemeDataset(data, tokenizer, image_dir, max_len, transform, has_labels)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=has_labels)\n\n# Define the Model\nclass MultiModalModel(nn.Module):\n    def __init__(self, visual_model, text_model, num_classes):\n        super(MultiModalModel, self).__init__()\n        self.visual_model = visual_model\n        self.visual_fc = nn.Linear(512, 768)\n        self.text_model = text_model\n        self.fc = nn.Sequential(\n            nn.Linear(768 + 768, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, images, input_ids, attention_mask):\n        visual_features = self.visual_model(images).squeeze(-1).squeeze(-1)\n        visual_features = self.visual_fc(visual_features)\n\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state.mean(dim=1)\n\n        combined_features = torch.cat((visual_features, text_features), dim=1)\n        return self.fc(combined_features)\n\n# Validation Function for Training\ndef validate_model(model, val_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    # Calculate Macro F1 Score\n    macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    return macro_f1\n\n# Validation Function for Test Set\ndef validate_test_set(model, test_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Validating Test Set\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(f\"Test Precision: {precision:.4f}\")\n    print(f\"Test Recall: {recall:.4f}\")\n    print(f\"Test F1 Score: {f1:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\n# Training Function\ndef train_model(model, train_loader, val_loader, epochs, optimizer, criterion):\n    model.to(device)\n    best_macro_f1 = 0\n    results = []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        macro_f1 = validate_model(model, val_loader)\n        results.append({\"Epoch\": epoch + 1, \"Loss\": total_loss / len(train_loader), \"Macro F1\": macro_f1})\n\n        if macro_f1 > best_macro_f1:\n            best_macro_f1 = macro_f1\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Macro F1: {macro_f1:.4f}\")\n\n    print(\"Training Complete!\")\n    return pd.DataFrame(results)\n\n# Main Function\ndef main():\n    # Paths\n    train_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/train.csv\"\n    train_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/images\"\n    dev_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/dev.csv\"\n    dev_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/images\"\n    test_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/test.csv\"\n    test_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/images\"\n\n    # Tokenizer and Hyperparameters\n    tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n    max_len = 128\n    batch_size = 16\n    epochs = 5\n    learning_rate = 3e-5\n\n    # Load Data\n    train_loader = load_data(train_csv, train_images, tokenizer, max_len, batch_size, has_labels=True)\n    val_loader = load_data(dev_csv, dev_images, tokenizer, max_len, batch_size, has_labels=True)\n    test_loader = load_data(test_csv, test_images, tokenizer, max_len, batch_size, has_labels=True)\n\n    # Initialize Model\n    visual_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n    visual_model = nn.Sequential(*list(visual_model.children())[:-1])\n    text_model = AutoModel.from_pretrained(\"ai4bharat/indic-bert\")\n    model = MultiModalModel(visual_model, text_model, num_classes=2)\n\n    # Loss and Optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n    # Train the Model\n    train_model(model, train_loader, val_loader, epochs, optimizer, criterion)\n\n    # Load Best Model\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n\n    # Validate on Test Set\n    validate_test_set(model, test_loader)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-25T21:00:39.891489Z","iopub.execute_input":"2025-01-25T21:00:39.891825Z","iopub.status.idle":"2025-01-25T21:03:56.977940Z","shell.execute_reply.started":"2025-01-25T21:00:39.891792Z","shell.execute_reply":"2025-01-25T21:03:56.977140Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nEpoch 1/5: 100%|██████████| 71/71 [00:31<00:00,  2.28it/s]\nValidating: 100%|██████████| 18/18 [00:06<00:00,  2.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.4988, Macro F1: 0.4799\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 71/71 [00:30<00:00,  2.31it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.3378, Macro F1: 0.7033\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 71/71 [00:30<00:00,  2.31it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.1425, Macro F1: 0.7183\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 71/71 [00:30<00:00,  2.32it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.0523, Macro F1: 0.7335\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 71/71 [00:30<00:00,  2.31it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.28it/s]\n<ipython-input-6-0cf9ca6969cb>:222: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.0348, Macro F1: 0.7496\nTraining Complete!\n","output_type":"stream"},{"name":"stderr","text":"Validating Test Set: 100%|██████████| 23/23 [00:08<00:00,  2.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Precision: 0.8095\nTest Recall: 0.7772\nTest F1 Score: 0.7909\nTest Accuracy: 0.8511\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install efficientnet_pytorch transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T04:57:08.020639Z","iopub.execute_input":"2025-01-27T04:57:08.020965Z","iopub.status.idle":"2025-01-27T04:57:12.088905Z","shell.execute_reply.started":"2025-01-27T04:57:08.020937Z","shell.execute_reply":"2025-01-27T04:57:12.087857Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.10/dist-packages (0.7.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->efficientnet_pytorch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom tqdm import tqdm\nfrom efficientnet_pytorch import EfficientNet\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Define Dataset Class\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, image_dir, max_len, transform, has_labels):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.image_dir = image_dir\n        self.max_len = max_len\n        self.transform = transform\n        self.has_labels = has_labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = str(self.data.loc[idx, 'image_id'])\n        if not img_name.endswith(\".jpg\"):\n            img_name += \".jpg\"\n        img_path = os.path.join(self.image_dir, img_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        caption = self.data.loc[idx, 'transcriptions']\n\n        if self.transform:\n            image = self.transform(image)\n\n        inputs = self.tokenizer(\n            caption, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len\n        )\n\n        item = {\n            'image': image,\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n        }\n\n        if self.has_labels:\n            item['label'] = int(self.data.loc[idx, 'labels'])\n\n        return item\n\n# Load Dataset Function\ndef load_data(csv_path, image_dir, tokenizer, max_len, batch_size, has_labels=True):\n    data = pd.read_csv(csv_path)\n\n    if has_labels:\n        data['labels'] = data['labels'].astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    dataset = MemeDataset(data, tokenizer, image_dir, max_len, transform, has_labels)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=has_labels)\n\n# Define the Model for LaBSE + EfficientNet-B4\nclass MultiModalModel(nn.Module):\n    def __init__(self, visual_model, text_model, num_classes):\n        super(MultiModalModel, self).__init__()\n        # Text Encoder (LaBSE)\n        self.text_model = text_model\n        self.text_fc = nn.Linear(768, 256)  # Reduce text features to 256 dimensions\n        \n        # Image Encoder (EfficientNet-B4)\n        self.visual_model = visual_model\n        self.visual_fc = nn.Linear(1792, 256)  # EfficientNet-B4 output is 1792\n        \n        # Classifier\n        self.fc = nn.Sequential(\n            nn.Linear(512, 128),  # Concatenated features (256 + 256)\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, images, input_ids, attention_mask):\n        # Text Features\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state.mean(dim=1)  # Average pooling\n        text_features = self.text_fc(text_features)\n        \n        # Image Features\n        image_features = self.visual_model.extract_features(images)\n        image_features = nn.functional.adaptive_avg_pool2d(image_features, (1, 1)).squeeze(-1).squeeze(-1)\n        image_features = self.visual_fc(image_features)\n        \n        # Concatenate and Classify\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        return self.fc(combined_features)\n\n# Validation Function for Training\ndef validate_model(model, val_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    # Calculate Metrics\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(f\"Validation Precision: {precision:.4f}\")\n    print(f\"Validation Recall: {recall:.4f}\")\n    print(f\"Validation F1 Score: {f1:.4f}\")\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\n# Validation Function for Test Set\ndef validate_test_set(model, test_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Validating Test Set\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    # Calculate Metrics\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(f\"Test Precision: {precision:.4f}\")\n    print(f\"Test Recall: {recall:.4f}\")\n    print(f\"Test F1 Score: {f1:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\n# Training Function\ndef train_model(model, train_loader, val_loader, epochs, optimizer, criterion):\n    model.to(device)\n    best_macro_f1 = 0\n    results = []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # Validate and Log Metrics\n        precision, recall, f1, accuracy = validate_model(model, val_loader)\n        results.append({\n            \"Epoch\": epoch + 1,\n            \"Loss\": total_loss / len(train_loader),\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"F1 Score\": f1,\n            \"Accuracy\": accuracy\n        })\n\n        if f1 > best_macro_f1:\n            best_macro_f1 = f1\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, F1: {f1:.4f}\")\n\n    print(\"Training Complete!\")\n    return pd.DataFrame(results)\n\n# Main Function\ndef main():\n    # Paths\n    train_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/train.csv\"\n    train_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/images\"\n    dev_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/dev.csv\"\n    dev_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/images\"\n    test_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/test.csv\"\n    test_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/images\"\n\n    # Tokenizer and Hyperparameters\n    tokenizer = AutoTokenizer.from_pretrained(\"setu4993/LaBSE\")  # LaBSE Tokenizer\n    max_len = 128\n    batch_size = 16\n    epochs = 5\n    learning_rate = 3e-5\n\n    # Load Data\n    train_loader = load_data(train_csv, train_images, tokenizer, max_len, batch_size, has_labels=True)\n    val_loader = load_data(dev_csv, dev_images, tokenizer, max_len, batch_size, has_labels=True)\n    test_loader = load_data(test_csv, test_images, tokenizer, max_len, batch_size, has_labels=True)\n\n    # Initialize Model\n    visual_model = EfficientNet.from_pretrained('efficientnet-b4')  # EfficientNet-B4\n    text_model = AutoModel.from_pretrained(\"setu4993/LaBSE\")  # LaBSE Model\n    model = MultiModalModel(visual_model, text_model, num_classes=2)\n\n    # Loss and Optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n    # Train the Model\n    train_model(model, train_loader, val_loader, epochs, optimizer, criterion)\n\n    # Load Best Model\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n\n    # Validate on Test Set\n    validate_test_set(model, test_loader)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:09:52.568388Z","iopub.execute_input":"2025-01-27T05:09:52.568774Z","iopub.status.idle":"2025-01-27T05:15:00.146428Z","shell.execute_reply.started":"2025-01-27T05:09:52.568743Z","shell.execute_reply":"2025-01-27T05:15:00.145669Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoaded pretrained weights for efficientnet-b4\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 71/71 [00:48<00:00,  1.46it/s]\nValidating: 100%|██████████| 18/18 [00:06<00:00,  2.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7283\nValidation Recall: 0.5847\nValidation F1 Score: 0.5863\nValidation Accuracy: 0.7676\nEpoch 1, Loss: 0.4990, F1: 0.5863\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 71/71 [00:48<00:00,  1.46it/s]\nValidating: 100%|██████████| 18/18 [00:06<00:00,  2.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.6682\nValidation Recall: 0.7085\nValidation F1 Score: 0.6722\nValidation Accuracy: 0.7113\nEpoch 2, Loss: 0.3666, F1: 0.6722\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 71/71 [00:48<00:00,  1.46it/s]\nValidating: 100%|██████████| 18/18 [00:06<00:00,  2.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.6817\nValidation Recall: 0.7017\nValidation F1 Score: 0.6891\nValidation Accuracy: 0.7465\nEpoch 3, Loss: 0.2439, F1: 0.6891\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 71/71 [00:48<00:00,  1.46it/s]\nValidating: 100%|██████████| 18/18 [00:06<00:00,  2.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7050\nValidation Recall: 0.6638\nValidation F1 Score: 0.6773\nValidation Accuracy: 0.7746\nEpoch 4, Loss: 0.1095, F1: 0.6773\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 71/71 [00:48<00:00,  1.47it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.6944\nValidation Recall: 0.6961\nValidation F1 Score: 0.6952\nValidation Accuracy: 0.7641\nEpoch 5, Loss: 0.0501, F1: 0.6952\nTraining Complete!\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-11-7fd28fd84c51>:251: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\"))\nValidating Test Set: 100%|██████████| 23/23 [00:07<00:00,  3.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Precision: 0.7687\nTest Recall: 0.7360\nTest F1 Score: 0.7494\nTest Accuracy: 0.8230\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nfrom transformers import XLMRobertaModel, XLMRobertaTokenizer, ViTModel, ViTFeatureExtractor\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom tqdm import tqdm\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# Define Dataset Class\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, image_dir, max_len, transform, has_labels):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.image_dir = image_dir\n        self.max_len = max_len\n        self.transform = transform\n        self.has_labels = has_labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = str(self.data.loc[idx, 'image_id'])\n        if not img_name.endswith(\".jpg\"):\n            img_name += \".jpg\"\n        img_path = os.path.join(self.image_dir, img_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        caption = self.data.loc[idx, 'transcriptions']\n\n        if self.transform:\n            image = self.transform(image)\n\n        inputs = self.tokenizer(\n            caption, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len\n        )\n\n        item = {\n            'image': image,\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n        }\n\n        if self.has_labels:\n            item['label'] = int(self.data.loc[idx, 'labels'])\n\n        return item\n\n# Load Dataset Function\ndef load_data(csv_path, image_dir, tokenizer, max_len, batch_size, has_labels=True):\n    data = pd.read_csv(csv_path)\n\n    if has_labels:\n        data['labels'] = data['labels'].astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    dataset = MemeDataset(data, tokenizer, image_dir, max_len, transform, has_labels)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=has_labels)\n\n# Define the Model for XLM-RoBERTa + ViT\nclass MultiModalModel(nn.Module):\n    def __init__(self, visual_model, text_model, num_classes):\n        super(MultiModalModel, self).__init__()\n        # Text Encoder (XLM-RoBERTa)\n        self.text_model = text_model\n        self.text_fc = nn.Linear(768, 256)  # Reduce text features to 256 dimensions\n        \n        # Image Encoder (ViT)\n        self.visual_model = visual_model\n        self.visual_fc = nn.Linear(768, 256)  # ViT output is 768\n        \n        # Classifier\n        self.fc = nn.Sequential(\n            nn.Linear(512, 128),  # Concatenated features (256 + 256)\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, images, input_ids, attention_mask):\n        # Text Features\n        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = text_outputs.last_hidden_state.mean(dim=1)  # Average pooling\n        text_features = self.text_fc(text_features)\n        \n        # Image Features\n        image_outputs = self.visual_model(pixel_values=images)\n        image_features = image_outputs.last_hidden_state.mean(dim=1)  # Average pooling\n        image_features = self.visual_fc(image_features)\n        \n        # Concatenate and Classify\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        return self.fc(combined_features)\n\n# Validation Function for Training\ndef validate_model(model, val_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    # Calculate Metrics\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(f\"Validation Precision: {precision:.4f}\")\n    print(f\"Validation Recall: {recall:.4f}\")\n    print(f\"Validation F1 Score: {f1:.4f}\")\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\n# Validation Function for Test Set\ndef validate_test_set(model, test_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Validating Test Set\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    # Calculate Metrics\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(f\"Test Precision: {precision:.4f}\")\n    print(f\"Test Recall: {recall:.4f}\")\n    print(f\"Test F1 Score: {f1:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\n# Training Function\ndef train_model(model, train_loader, val_loader, epochs, optimizer, criterion):\n    model.to(device)\n    best_macro_f1 = 0\n    results = []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images, input_ids, attention_mask)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # Validate and Log Metrics\n        precision, recall, f1, accuracy = validate_model(model, val_loader)\n        results.append({\n            \"Epoch\": epoch + 1,\n            \"Loss\": total_loss / len(train_loader),\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"F1 Score\": f1,\n            \"Accuracy\": accuracy\n        })\n\n        if f1 > best_macro_f1:\n            best_macro_f1 = f1\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, F1: {f1:.4f}\")\n\n    print(\"Training Complete!\")\n    return pd.DataFrame(results)\n\n# Main Function\ndef main():\n    # Paths\n    train_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/train.csv\"\n    train_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/images\"\n    dev_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/dev.csv\"\n    dev_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/images\"\n    test_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/test.csv\"\n    test_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/images\"\n\n    # Tokenizer and Hyperparameters\n    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")  # XLM-RoBERTa Tokenizer\n    max_len = 128\n    batch_size = 16\n    epochs = 5\n    learning_rate = 3e-5\n\n    # Load Data\n    train_loader = load_data(train_csv, train_images, tokenizer, max_len, batch_size, has_labels=True)\n    val_loader = load_data(dev_csv, dev_images, tokenizer, max_len, batch_size, has_labels=True)\n    test_loader = load_data(test_csv, test_images, tokenizer, max_len, batch_size, has_labels=True)\n\n    # Initialize Model\n    visual_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")  # ViT Model\n    text_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")  # XLM-RoBERTa Model\n    model = MultiModalModel(visual_model, text_model, num_classes=2)\n\n    # Loss and Optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n    # Train the Model\n    train_model(model, train_loader, val_loader, epochs, optimizer, criterion)\n\n    # Load Best Model\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n\n    # Validate on Test Set\n    validate_test_set(model, test_loader)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:37:08.501431Z","iopub.execute_input":"2025-01-27T05:37:08.501773Z","iopub.status.idle":"2025-01-27T05:42:41.690833Z","shell.execute_reply.started":"2025-01-27T05:37:08.501749Z","shell.execute_reply":"2025-01-27T05:42:41.690072Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/5: 100%|██████████| 71/71 [00:54<00:00,  1.29it/s]\nValidating: 100%|██████████| 18/18 [00:07<00:00,  2.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7104\nValidation Recall: 0.6376\nValidation F1 Score: 0.6533\nValidation Accuracy: 0.7746\nEpoch 1, Loss: 0.4886, F1: 0.6533\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 71/71 [00:53<00:00,  1.33it/s]\nValidating: 100%|██████████| 18/18 [00:06<00:00,  2.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7521\nValidation Recall: 0.6272\nValidation F1 Score: 0.6436\nValidation Accuracy: 0.7852\nEpoch 2, Loss: 0.3064, F1: 0.6436\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 71/71 [00:53<00:00,  1.33it/s]\nValidating: 100%|██████████| 18/18 [00:06<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7248\nValidation Recall: 0.6666\nValidation F1 Score: 0.6835\nValidation Accuracy: 0.7852\nEpoch 3, Loss: 0.1503, F1: 0.6835\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 71/71 [00:53<00:00,  1.32it/s]\nValidating: 100%|██████████| 18/18 [00:07<00:00,  2.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.8245\nValidation Recall: 0.6638\nValidation F1 Score: 0.6904\nValidation Accuracy: 0.8134\nEpoch 4, Loss: 0.0748, F1: 0.6904\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 71/71 [00:54<00:00,  1.31it/s]\nValidating: 100%|██████████| 18/18 [00:07<00:00,  2.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7848\nValidation Recall: 0.6789\nValidation F1 Score: 0.7037\nValidation Accuracy: 0.8099\nEpoch 5, Loss: 0.0230, F1: 0.7037\nTraining Complete!\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-12-0f509550770c>:250: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\"))\nValidating Test Set: 100%|██████████| 23/23 [00:08<00:00,  2.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Precision: 0.8240\nTest Recall: 0.7453\nTest F1 Score: 0.7720\nTest Accuracy: 0.8483\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom transformers import BertModel, BertTokenizer\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom tqdm import tqdm\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, image_dir, max_len, transform, has_labels):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.image_dir = image_dir\n        self.max_len = max_len\n        self.transform = transform\n        self.has_labels = has_labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = str(self.data.loc[idx, 'image_id'])\n        if not img_name.endswith(\".jpg\"):\n            img_name += \".jpg\"\n        img_path = os.path.join(self.image_dir, img_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        caption = self.data.loc[idx, 'transcriptions']\n\n        if self.transform:\n            image = self.transform(image)\n\n        inputs = self.tokenizer(\n            caption,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n\n        item = {\n            'image': image,\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n            'token_type_ids': inputs['token_type_ids'].squeeze()\n        }\n\n        if self.has_labels:\n            item['label'] = int(self.data.loc[idx, 'labels'])\n\n        return item\n\ndef load_data(csv_path, image_dir, tokenizer, max_len, batch_size, has_labels=True):\n    data = pd.read_csv(csv_path)\n\n    if has_labels:\n        data['labels'] = data['labels'].astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    dataset = MemeDataset(data, tokenizer, image_dir, max_len, transform, has_labels)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=has_labels)\n\nclass MultiModalModel(nn.Module):\n    def __init__(self, num_classes):\n        super(MultiModalModel, self).__init__()\n        # Text Encoder (mBERT)\n        self.text_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n        self.text_fc = nn.Linear(768, 256)\n        \n        # Image Encoder (ResNet-50)\n        self.visual_model = models.resnet50(pretrained=True)\n        self.visual_model = nn.Sequential(*list(self.visual_model.children())[:-1])\n        self.visual_fc = nn.Linear(2048, 256)\n        \n        # Classifier\n        self.fc = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, images, input_ids, attention_mask, token_type_ids):\n        # Text Features\n        text_outputs = self.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        text_features = text_outputs.last_hidden_state.mean(dim=1)\n        text_features = self.text_fc(text_features)\n        \n        # Image Features\n        image_features = self.visual_model(images)\n        image_features = image_features.squeeze(-1).squeeze(-1)\n        image_features = self.visual_fc(image_features)\n        \n        # Concatenate and Classify\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        return self.fc(combined_features)\n\ndef validate_model(model, val_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask, token_type_ids)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(f\"Validation Precision: {precision:.4f}\")\n    print(f\"Validation Recall: {recall:.4f}\")\n    print(f\"Validation F1 Score: {f1:.4f}\")\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\ndef evaluate_test_set(model, test_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n    test_loss = 0\n    criterion = nn.CrossEntropyLoss()\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask, token_type_ids)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            \n            preds = outputs.argmax(dim=1).cpu().numpy()\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    # Calculate metrics\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n    # Calculate per-class metrics\n    class_precision = precision_score(all_labels, all_preds, average=None)\n    class_recall = recall_score(all_labels, all_preds, average=None)\n    class_f1 = f1_score(all_labels, all_preds, average=None)\n\n    # Print results\n    print(\"\\n=== Test Set Evaluation Results ===\")\n    print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n    print(f\"\\nOverall Metrics:\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Macro Precision: {precision:.4f}\")\n    print(f\"Macro Recall: {recall:.4f}\")\n    print(f\"Macro F1 Score: {f1:.4f}\")\n    \n    print(\"\\nPer-Class Metrics:\")\n    for i in range(len(class_precision)):\n        print(f\"\\nClass {i}:\")\n        print(f\"Precision: {class_precision[i]:.4f}\")\n        print(f\"Recall: {class_recall[i]:.4f}\")\n        print(f\"F1 Score: {class_f1[i]:.4f}\")\n\n    return {\n        'test_loss': test_loss/len(test_loader),\n        'accuracy': accuracy,\n        'macro_precision': precision,\n        'macro_recall': recall,\n        'macro_f1': f1,\n        'class_precision': class_precision.tolist(),\n        'class_recall': class_recall.tolist(),\n        'class_f1': class_f1.tolist()\n    }\n\ndef train_model(model, train_loader, val_loader, epochs, optimizer, criterion):\n    model.to(device)\n    best_macro_f1 = 0\n    results = []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images, input_ids, attention_mask, token_type_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # Validate and Log Metrics\n        precision, recall, f1, accuracy = validate_model(model, val_loader)\n        results.append({\n            \"Epoch\": epoch + 1,\n            \"Loss\": total_loss / len(train_loader),\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"F1 Score\": f1,\n            \"Accuracy\": accuracy\n        })\n\n        if f1 > best_macro_f1:\n            best_macro_f1 = f1\n            torch.save(model.state_dict(), \"best_model.pth\")\n\n        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, F1: {f1:.4f}\")\n\n    return pd.DataFrame(results)\n\ndef main():\n    # Paths\n    train_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/train.csv\"\n    train_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/images\"\n    dev_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/dev.csv\"\n    dev_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/images\"\n    test_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/test.csv\"\n    test_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/images\"\n\n    # Hyperparameters\n    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    max_len = 128\n    batch_size = 16\n    epochs = 5\n    learning_rate = 3e-5\n\n    # Load Data\n    print(\"Loading datasets...\")\n    train_loader = load_data(train_csv, train_images, tokenizer, max_len, batch_size)\n    val_loader = load_data(dev_csv, dev_images, tokenizer, max_len, batch_size)\n    test_loader = load_data(test_csv, test_images, tokenizer, max_len, batch_size)\n\n    # Initialize Model\n    print(\"Initializing model...\")\n    model = MultiModalModel(num_classes=2)\n\n    # Loss and Optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n    # Train the Model\n    print(\"Starting training...\")\n    results_df = train_model(model, train_loader, val_loader, epochs, optimizer, criterion)\n    \n    # Save training results\n    results_df.to_csv('training_results.csv', index=False)\n    \n    # Load best model and evaluate on test set\n    print(\"\\nLoading best model for testing...\")\n    model.load_state_dict(torch.load('best_model.pth'))\n    \n    # Final evaluation on test set\n    print(\"Evaluating on test set...\")\n    test_metrics = evaluate_test_set(model, test_loader)\n    \n    # Save test metrics\n    test_results_df = pd.DataFrame([test_metrics])\n    test_results_df.to_csv('test_results.csv', index=False)\n    \n    return results_df, test_metrics\n\nif __name__ == \"__main__\":\n    train_results, test_metrics = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T05:43:38.113540Z","iopub.execute_input":"2025-01-27T05:43:38.113847Z","iopub.status.idle":"2025-01-27T05:47:39.368915Z","shell.execute_reply.started":"2025-01-27T05:43:38.113825Z","shell.execute_reply":"2025-01-27T05:47:39.368017Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec0580b3ebe644edb490bcd358041889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df229edaa557485d82d14d371af2363e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b1f7d4e6bfc4baa93f9b93b385a7803"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b63f67f589644f3985c21cb203c34ce"}},"metadata":{}},{"name":"stdout","text":"Loading datasets...\nInitializing model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c38a13d37c843d0a18acc15adaab754"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 157MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 71/71 [00:38<00:00,  1.86it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7665\nValidation Recall: 0.5604\nValidation F1 Score: 0.5453\nValidation Accuracy: 0.7641\nEpoch 1, Loss: 0.4747, F1: 0.5453\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 71/71 [00:37<00:00,  1.89it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7966\nValidation Recall: 0.6411\nValidation F1 Score: 0.6620\nValidation Accuracy: 0.7993\nEpoch 2, Loss: 0.2679, F1: 0.6620\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 71/71 [00:37<00:00,  1.88it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7865\nValidation Recall: 0.7123\nValidation F1 Score: 0.7354\nValidation Accuracy: 0.8204\nEpoch 3, Loss: 0.0742, F1: 0.7354\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 71/71 [00:37<00:00,  1.88it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7469\nValidation Recall: 0.7135\nValidation F1 Score: 0.7265\nValidation Accuracy: 0.8028\nEpoch 4, Loss: 0.0265, F1: 0.7265\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 71/71 [00:37<00:00,  1.87it/s]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7680\nValidation Recall: 0.7318\nValidation F1 Score: 0.7461\nValidation Accuracy: 0.8169\nEpoch 5, Loss: 0.0398, F1: 0.7461\n\nLoading best model for testing...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-13-b1928ea557b6>:288: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|██████████| 23/23 [00:07<00:00,  3.24it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== Test Set Evaluation Results ===\nTest Loss: 0.4050\n\nOverall Metrics:\nAccuracy: 0.8624\nMacro Precision: 0.8209\nMacro Recall: 0.8034\nMacro F1 Score: 0.8114\n\nPer-Class Metrics:\n\nClass 0:\nPrecision: 0.8978\nRecall: 0.9213\nF1 Score: 0.9094\n\nClass 1:\nPrecision: 0.7439\nRecall: 0.6854\nF1 Score: 0.7135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom tqdm import tqdm\n\n# Device Configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nclass MemeDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, image_dir, max_len, transform, has_labels):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.image_dir = image_dir\n        self.max_len = max_len\n        self.transform = transform\n        self.has_labels = has_labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = str(self.data.loc[idx, 'image_id'])\n        if not img_name.endswith(\".jpg\"):\n            img_name += \".jpg\"\n        img_path = os.path.join(self.image_dir, img_name)\n\n        image = Image.open(img_path).convert(\"RGB\")\n        caption = self.data.loc[idx, 'transcriptions']\n\n        if self.transform:\n            image = self.transform(image)\n\n        inputs = self.tokenizer(\n            caption,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n\n        item = {\n            'image': image,\n            'input_ids': inputs['input_ids'].squeeze(),\n            'attention_mask': inputs['attention_mask'].squeeze(),\n            'token_type_ids': inputs['token_type_ids'].squeeze()\n        }\n\n        if self.has_labels:\n            item['label'] = int(self.data.loc[idx, 'labels'])\n\n        return item\n\ndef load_data(csv_path, image_dir, tokenizer, max_len, batch_size, has_labels=True):\n    data = pd.read_csv(csv_path)\n\n    if has_labels:\n        data['labels'] = data['labels'].astype(int)\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    dataset = MemeDataset(data, tokenizer, image_dir, max_len, transform, has_labels)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=has_labels)\n\nclass MultiModalModel(nn.Module):\n    def __init__(self, num_classes):\n        super(MultiModalModel, self).__init__()\n        self.text_model = AutoModel.from_pretrained(\"google/muril-base-cased\")\n        self.text_fc = nn.Linear(768, 256)\n        \n        self.visual_model = models.resnet50(pretrained=True)\n        self.visual_model = nn.Sequential(*list(self.visual_model.children())[:-1])\n        self.visual_fc = nn.Linear(2048, 256)\n        \n        self.fusion = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, images, input_ids, attention_mask, token_type_ids):\n        text_outputs = self.text_model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        text_features = text_outputs.last_hidden_state.mean(dim=1)\n        text_features = self.text_fc(text_features)\n        \n        image_features = self.visual_model(images)\n        image_features = image_features.squeeze(-1).squeeze(-1)\n        image_features = self.visual_fc(image_features)\n        \n        combined_features = torch.cat((text_features, image_features), dim=1)\n        fused_features = self.fusion(combined_features)\n        output = self.classifier(fused_features)\n        return output\n\ndef validate_model(model, val_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask, token_type_ids)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(f\"Validation Precision: {precision:.4f}\")\n    print(f\"Validation Recall: {recall:.4f}\")\n    print(f\"Validation F1 Score: {f1:.4f}\")\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\ndef evaluate_model(model, test_loader):\n    model.eval()\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating Test Data\"):\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(images, input_ids, attention_mask, token_type_ids)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(preds)\n\n    precision = precision_score(all_labels, all_preds, average=\"macro\")\n    recall = recall_score(all_labels, all_preds, average=\"macro\")\n    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n    accuracy = accuracy_score(all_labels, all_preds)\n\n    print(\"\\nTest Evaluation Metrics:\")\n    print(f\"Test Precision: {precision:.4f}\")\n    print(f\"Test Recall: {recall:.4f}\")\n    print(f\"Test F1 Score: {f1:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n\n    return precision, recall, f1, accuracy\n\ndef train_model(model, train_loader, val_loader, epochs, optimizer, criterion, scheduler=None):\n    model.to(device)\n    best_macro_f1 = 0\n    results = []\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n\n        for batch in progress_bar:\n            images = batch['image'].to(device)\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images, input_ids, attention_mask, token_type_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            if scheduler is not None:\n                scheduler.step()\n\n            total_loss += loss.item()\n            progress_bar.set_postfix({'training_loss': f'{loss.item():.3f}'})\n\n        avg_loss = total_loss / len(train_loader)\n        precision, recall, f1, accuracy = validate_model(model, val_loader)\n        \n        results.append({\n            \"Epoch\": epoch + 1,\n            \"Loss\": avg_loss,\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"F1 Score\": f1,\n            \"Accuracy\": accuracy\n        })\n\n        if f1 > best_macro_f1:\n            best_macro_f1 = f1\n            torch.save(model.state_dict(), \"best_model.pth\")\n            print(f\"New best model saved with F1: {f1:.4f}\")\n\n        print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}, F1: {f1:.4f}\")\n\n    return pd.DataFrame(results)\n\ndef main():\n    # Paths\n    train_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/train.csv\"\n    train_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/train/images\"\n    dev_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/dev.csv\"\n    dev_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/dev/images\"\n    test_csv = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/test.csv\"\n    test_images = \"/kaggle/input/tamil-labelled-dataset/Dataset with label/test/images\"\n\n    # Hyperparameters\n    tokenizer = AutoTokenizer.from_pretrained(\"google/muril-base-cased\")\n    max_len = 128\n    batch_size = 16\n    epochs = 5\n    learning_rate = 2e-5\n\n    # Load Data\n    print(\"Loading datasets...\")\n    train_loader = load_data(train_csv, train_images, tokenizer, max_len, batch_size)\n    val_loader = load_data(dev_csv, dev_images, tokenizer, max_len, batch_size)\n    test_loader = load_data(test_csv, test_images, tokenizer, max_len, batch_size)\n\n    # Initialize Model\n    print(\"Initializing model...\")\n    model = MultiModalModel(num_classes=2)\n\n    # Optimizer and Scheduler\n    optimizer = torch.optim.AdamW([\n        {'params': model.text_model.parameters(), 'lr': learning_rate},\n        {'params': model.visual_model.parameters(), 'lr': learning_rate * 10},\n        {'params': list(model.text_fc.parameters()) + \n                  list(model.visual_fc.parameters()) + \n                  list(model.fusion.parameters()) + \n                  list(model.classifier.parameters()), \n         'lr': learning_rate * 100}\n    ])\n\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=[learning_rate, learning_rate * 10, learning_rate * 100],\n        steps_per_epoch=len(train_loader),\n        epochs=epochs\n    )\n\n    criterion = nn.CrossEntropyLoss()\n\n    # Train Model\n    print(\"Starting training...\")\n    results_df = train_model(model, train_loader, val_loader, epochs, optimizer, criterion, scheduler)\n    results_df.to_csv('training_results.csv', index=False)\n\n    # Load Best Model and Evaluate on Test Data\n    print(\"Evaluating on test data...\")\n    model.load_state_dict(torch.load(\"best_model.pth\"))\n    evaluate_model(model, test_loader)\n\n    return results_df\n\nif __name__ == \"__main__\":\n    results = main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T06:35:23.048720Z","iopub.execute_input":"2025-01-27T06:35:23.049033Z","iopub.status.idle":"2025-01-27T06:39:24.720762Z","shell.execute_reply.started":"2025-01-27T06:35:23.049009Z","shell.execute_reply":"2025-01-27T06:39:24.719810Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading datasets...\nInitializing model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 71/71 [00:39<00:00,  1.79it/s, training_loss=0.218]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.8551\nValidation Recall: 0.5922\nValidation F1 Score: 0.5936\nValidation Accuracy: 0.7852\nNew best model saved with F1: 0.5936\nEpoch 1, Loss: 0.5104, F1: 0.5936\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 71/71 [00:38<00:00,  1.86it/s, training_loss=0.498]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7707\nValidation Recall: 0.5962\nValidation F1 Score: 0.6018\nValidation Accuracy: 0.7782\nNew best model saved with F1: 0.6018\nEpoch 2, Loss: 0.4363, F1: 0.6018\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 71/71 [00:37<00:00,  1.88it/s, training_loss=0.357]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.7016\nValidation Recall: 0.7223\nValidation F1 Score: 0.7096\nValidation Accuracy: 0.7641\nNew best model saved with F1: 0.7096\nEpoch 3, Loss: 0.3534, F1: 0.7096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 71/71 [00:38<00:00,  1.85it/s, training_loss=0.025]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.8152\nValidation Recall: 0.7373\nValidation F1 Score: 0.7627\nValidation Accuracy: 0.8380\nNew best model saved with F1: 0.7627\nEpoch 4, Loss: 0.1854, F1: 0.7627\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 71/71 [00:38<00:00,  1.86it/s, training_loss=0.132]\nValidating: 100%|██████████| 18/18 [00:05<00:00,  3.24it/s]\n<ipython-input-16-a15be724066e>:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"best_model.pth\"))\n","output_type":"stream"},{"name":"stdout","text":"Validation Precision: 0.8112\nValidation Recall: 0.7306\nValidation F1 Score: 0.7561\nValidation Accuracy: 0.8345\nEpoch 5, Loss: 0.0585, F1: 0.7561\nEvaluating on test data...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Test Data: 100%|██████████| 23/23 [00:07<00:00,  3.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nTest Evaluation Metrics:\nTest Precision: 0.7712\nTest Recall: 0.7285\nTest F1 Score: 0.7449\nTest Accuracy: 0.8230\n","output_type":"stream"}],"execution_count":16}]}